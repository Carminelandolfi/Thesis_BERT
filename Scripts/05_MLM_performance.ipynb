{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "\n",
    "\n",
    "In this section, we import all the necessary libraries for data preprocessing, tokenization, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSRAUIuJtcDp"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The abstracts are first split into sentences and tokenized with BERT’s tokenizer, adding the special tokens `[CLS]` at the start and `[SEP]` at the end of each sentence.\n",
    "\n",
    "Since BERT has a maximum sequence length of 512 tokens, we segment longer token sequences into overlapping chunks of 512 tokens, with a stride of 462 tokens (512 - 50) to maintain some context overlap between segments.\n",
    "\n",
    "Each chunk is padded to the maximum length, and attention masks and token type IDs are created accordingly. The output is a dictionary of tensors ready for input into the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2fIO0-XwXFD"
   },
   "outputs": [],
   "source": [
    "class TokenizerSegmenter:\n",
    "    \"\"\"\n",
    "    A utility class for segmenting and tokenizing textual abstracts into\n",
    "    overlapping BERT-compatible input chunks.\n",
    "\n",
    "    This is especially useful for handling long abstracts that exceed the BERT\n",
    "    maximum input length (typically 512 tokens). It applies a sliding window\n",
    "    approach with a defined stride to create multiple segments per abstract,\n",
    "    ensuring coverage while preserving sentence boundaries.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (BertTokenizer): Pretrained BERT tokenizer for tokenizing text.\n",
    "        max_length (int): Maximum sequence length for BERT input (default is 512).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        Initializes the TokenizerSegmenter.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (BertTokenizer): The tokenizer used to encode the abstracts.\n",
    "            max_length (int): Maximum length of tokenized input sequences.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def process(self, abstracts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of abstracts into padded, tokenized BERT-compatible inputs\n",
    "        using a sliding window strategy.\n",
    "\n",
    "        Args:\n",
    "            abstracts (List[str]): List of raw abstract texts to be tokenized and segmented.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing:\n",
    "                - 'input_ids' (Tensor): Padded token IDs for each segment.\n",
    "                - 'attention_mask' (Tensor): Attention masks for each segment.\n",
    "                - 'token_type_ids' (Tensor): Segment token type IDs (all zeros).\n",
    "        \"\"\"\n",
    "        input_ids_all, attention_masks_all, token_type_ids_all = [], [], []\n",
    "        stride = self.max_length - 50\n",
    "        cls_id = self.tokenizer.cls_token_id\n",
    "        sep_id = self.tokenizer.sep_token_id\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "        for abstract in abstracts:\n",
    "            # Tokenize full abstract at sentence level\n",
    "            sentences = sent_tokenize(abstract)\n",
    "            token_ids = [cls_id]\n",
    "    \n",
    "            for sent in sentences:\n",
    "                sent_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n",
    "                token_ids.extend(sent_ids + [sep_id])\n",
    "\n",
    "            for i in range(0, len(token_ids), stride):\n",
    "                chunk = token_ids[i:i + self.max_length]\n",
    "                len_chunk = len(chunk)\n",
    "\n",
    "               # Ensure [CLS] at start and [SEP] at end\n",
    "                if chunk[0] != cls_id:\n",
    "                    chunk = [cls_id] + chunk[:len_chunk - 1]\n",
    "                if chunk[-1] != sep_id:\n",
    "                    chunk[-1] = sep_id\n",
    "\n",
    "                pad_len = self.max_length - len_chunk\n",
    "\n",
    "                chunk_padded = chunk + [self.tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask = [1] * len_chunk + [0] * pad_len\n",
    "                token_type_ids = [0] * self.max_length\n",
    "\n",
    "                input_ids_all.append(chunk_padded)\n",
    "                attention_masks_all.append(attention_mask)\n",
    "                token_type_ids_all.append(token_type_ids)\n",
    "                i += stride\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids_all, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_masks_all, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids_all, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwouNdAx32nt"
   },
   "outputs": [],
   "source": [
    "class BERTPretrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for BERT-style pretraining tasks, specifically designed\n",
    "    to handle inputs for Masked Language Modeling (MLM).\n",
    "\n",
    "    This dataset is expected to receive pre-tokenized and pre-processed inputs,\n",
    "    including input_ids, token_type_ids, attention_mask, labels (for MLM).\n",
    "\n",
    "    Attributes:\n",
    "        inputs (Dict[str, torch.Tensor]): A dictionary containing all input fields required by BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with input tensors.\n",
    "\n",
    "        Args:\n",
    "            inputs (Dict[str, torch.Tensor]): Dictionary with keys:\n",
    "                - 'input_ids': Token IDs tensor of shape (N, seq_len)\n",
    "                - 'token_type_ids': Segment type IDs tensor\n",
    "                - 'attention_mask': Attention mask tensor\n",
    "                - 'labels': MLM labels tensor (-100 for non-masked tokens)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.inputs['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetches the input sample at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing a single sample:\n",
    "                - 'input_ids': Tensor\n",
    "                - 'token_type_ids': Tensor\n",
    "                - 'attention_mask': Tensor\n",
    "                - 'labels': Tensor\n",
    "        \"\"\"\n",
    "        return {key: self.inputs[key][idx] for key in self.inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking of dataset\n",
    "\n",
    "In order to evaluate the model’s performance on the Masked Language Modeling (MLM) task, we apply three different masking strategies only during the inference phase. In all cases, special tokens such as [CLS], [SEP], and padding are excluded from masking:\n",
    "\n",
    "- Random masking: 15% of the tokens are randomly selected and replaced with the [MASK] token (ID 103), following the standard BERT procedure.\n",
    "\n",
    "- Domain-specific masking: only technical or scientific terms—such as named entities or specialized vocabulary—are masked. This allows us to assess how well the model has learned domain-specific knowledge.\n",
    "\n",
    "- Non-technical masking: only common or non-domain-specific terms (e.g., stop words or general vocabulary) are masked. This strategy helps evaluate the model’s grasp of general language structure and context.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XnhqUCswuDi"
   },
   "outputs": [],
   "source": [
    "class BERTMasker:\n",
    "    \"\"\"\n",
    "    Utility class for applying BERT-style masking to input token IDs\n",
    "    for the Masked Language Modeling (MLM) objective.\n",
    "\n",
    "    This class performs dynamic masking following the original BERT paper:\n",
    "        - 80% of the time: replace token with [MASK]\n",
    "        - 10% of the time: replace token with a random token\n",
    "        - 10% of the time: keep token unchanged\n",
    "\n",
    "    Special tokens such as [CLS], [SEP], and [PAD] are never masked.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (BertTokenizer): A pretrained BERT tokenizer providing special token IDs.\n",
    "        mask_prob (float): Probability of masking each token (default: 0.15).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BertTokenizer, mask_prob: float = 0.15):\n",
    "        \"\"\"\n",
    "        Initializes the BERTMasker with the tokenizer and masking probability.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (BertTokenizer): The tokenizer used for accessing token IDs.\n",
    "            mask_prob (float): Probability of masking each token (default: 15%).\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def apply_masking(self, input_ids: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Applies BERT-style masking to the input tensor of token IDs.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Tensor of shape (batch_size, seq_length) containing token IDs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - masked_input_ids (Tensor): Input tensor with some tokens replaced with [MASK], random, or unchanged.\n",
    "                - labels (Tensor): Target labels for MLM loss computation, with -100 for unmasked positions.\n",
    "        \"\"\"\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        labels = input_ids.clone()\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "\n",
    "        # Create boolean mask for which tokens will be masked\n",
    "        mask_arr = (\n",
    "            (rand < self.mask_prob) &\n",
    "            (input_ids != self.tokenizer.cls_token_id) &\n",
    "            (input_ids != self.tokenizer.sep_token_id) &\n",
    "            (input_ids != self.tokenizer.pad_token_id)\n",
    "        )\n",
    "\n",
    "        labels[~mask_arr] = -100  # Only compute loss on masked tokens\n",
    "\n",
    "        for i in range(mask_arr.shape[0]):\n",
    "            token_indices = torch.nonzero(mask_arr[i]).flatten().tolist()\n",
    "            for token_idx in token_indices:\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    masked_input_ids[i, token_idx] = self.tokenizer.mask_token_id  # Replace with [MASK]\n",
    "                elif prob < 0.9:\n",
    "                    masked_input_ids[i, token_idx] = random.randint(0, self.tokenizer.vocab_size - 1)  # Random token\n",
    "                # Else: leave unchanged\n",
    "\n",
    "        return masked_input_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhdXXZrgwyad"
   },
   "outputs": [],
   "source": [
    "class MLMEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates a BERT model on the Masked Language Modeling (MLM) objective.\n",
    "\n",
    "    This class provides utility functions to:\n",
    "      - Compute the loss and logits on a given batch.\n",
    "      - Extract the top-k predicted token probabilities from the model's output.\n",
    "\n",
    "    Attributes:\n",
    "        model (BertForMaskedLM): A pretrained or fine-tuned BERT model for MLM evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: BertForMaskedLM):\n",
    "        \"\"\"\n",
    "        Initializes the evaluator with a BERT MLM model.\n",
    "\n",
    "        Args:\n",
    "            model (BertForMaskedLM): The model to evaluate.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    def evaluate(self, features: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on the provided input features.\n",
    "\n",
    "        Args:\n",
    "            features (Dict[str, Tensor]): A dictionary containing input tensors:\n",
    "                - 'input_ids': Tensor of token IDs.\n",
    "                - 'attention_mask': Tensor indicating attention (non-padding).\n",
    "                - 'token_type_ids': Tensor of segment IDs (for NSP compatibility).\n",
    "                - 'labels': Tensor of target token IDs with -100 for ignored positions.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: A dictionary with:\n",
    "                - 'loss': Cross-entropy loss over masked tokens.\n",
    "                - 'logits': Raw prediction scores from the model.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=features[\"input_ids\"],\n",
    "                attention_mask=features[\"attention_mask\"],\n",
    "                token_type_ids=features[\"token_type_ids\"],\n",
    "                labels=features[\"labels\"]\n",
    "            )\n",
    "            return {\n",
    "                \"loss\": outputs.loss,\n",
    "                \"logits\": outputs.logits\n",
    "            }\n",
    "\n",
    "   \n",
    "    def get_token_probabilities(self, logits: Tensor, top_k: int = 5) -> List[List[Tuple[int, float]]]:\n",
    "        \"\"\"\n",
    "        Extracts the top-k token predictions for each position in each sequence.\n",
    "    \n",
    "        Args:\n",
    "            logits (Tensor): The model's output logits of shape (batch_size, seq_len, vocab_size).\n",
    "            top_k (int): Number of top predictions to return per token position.\n",
    "    \n",
    "        Returns:\n",
    "            List[List[Tuple[int, float]]]: Nested list of (token_id, probability) per token position.\n",
    "        \"\"\"\n",
    "        top_probs, top_indices = torch.topk(torch.softmax(logits, dim=-1), k=top_k, dim=-1)\n",
    "    \n",
    "        return [\n",
    "            [(int(tok_id), float(prob)) for tok_id, prob in zip(pos_indices, pos_probs)]\n",
    "            for pos_indices, pos_probs in zip(top_indices.view(-1, top_k), top_probs.view(-1, top_k))\n",
    "        ]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTXubmr92HE2"
   },
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Preprocessing\n",
    "segmenter = TokenizerSegmenter(tokenizer)\n",
    "masker = BERTMasker(tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"../Datasets/dataset_20k_val.csv\")\n",
    "features = segmenter.process(df[\"abstract_clean\"])\n",
    "features[\"input_ids\"], features[\"labels\"] = masker.apply_masking(features[\"input_ids\"])\n",
    "\n",
    "# 3. Dataset e DataLoader\n",
    "dataset = BERTPretrainingDataset(features)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 4. Evaluazione\n",
    "evaluator = MLMEvaluator(model)\n",
    "results = evaluator.evaluate_batches(dataloader, device)\n",
    "\n",
    "print(\"Average loss:\", results[\"average_loss\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKEDnSsc0lzvEE4Wxemqj8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
