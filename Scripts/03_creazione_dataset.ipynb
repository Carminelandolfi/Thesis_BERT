{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e647a6e-8721-4703-8057-29472a86d0df",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "\n",
    "In this section, we import all the necessary libraries for data preprocessing, tokenization, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9117a4a0-340d-4b42-a6cb-97f9a9d53f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf642ab-532b-4be3-be8b-1910a97f4b46",
   "metadata": {},
   "source": [
    "# 2. Dataset Loading\n",
    "\n",
    "In this section, we load the dataset of scientific abstracts that were previously collected and cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a731f7f7-6f5c-4e01-afd0-7dc4a6fb788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/cs_papers_api.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec051ac-9db6-4741-9594-68a8ef9126a2",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "The following datasets have been prepared for various stages of the experimental pipeline:\n",
    "\n",
    "- **Domain-Adaptive Pretraining (DAPT):**  \n",
    "  Creation of three datasets consisting of:\n",
    "  - 10K abstracts  \n",
    "  - 50K abstracts  \n",
    "  - 100K abstracts  \n",
    "\n",
    "- **Masked Language Modeling (MLM) Evaluation:**  \n",
    "  A separate dataset of 20K abstracts, not overlapping with the DAPT datasets, used to evaluate the model's performance on the MLM task.\n",
    "\n",
    "- **Fine-Tuning:**  \n",
    "  A residual dataset, composed of all remaining abstracts not used in DAPT or MLM evaluation, reserved for downstream fine-tuning tasks (e.g., classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafd2af-b92c-4406-95fe-6b4fc2a6ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "DAPT_SAMPLE_SIZE = 100_000      # Total abstracts to sample for main dataset\n",
    "SAMPLE_10K = 10_000             # Size of smaller subset from 100K\n",
    "SAMPLE_50K = 50_000             # Size of larger subset from 100K \n",
    "VAL_SAMPLE_SIZE = 20_000        # Validation dataset size from outside the 100K\n",
    "SEED = 123                     # Random seed for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "N = len(dataset)  # Total number of abstracts available\n",
    "\n",
    "# Step 1: Randomly sample 100K abstracts for the main domain-adaptive pretraining dataset\n",
    "indices_100k = set(random.sample(range(N), DAPT_SAMPLE_SIZE))\n",
    "\n",
    "# Step 2: From the 100K dataset, randomly select 10K abstracts\n",
    "indices_10k = set(random.sample(indices_100k, SAMPLE_10K))\n",
    "\n",
    "# Step 3: From the remaining 90K (100K minus 10K), select 50K abstracts\n",
    "indices_50k = set(random.sample(indices_100k, SAMPLE_50K))\n",
    "\n",
    "# Step 4: From the abstracts NOT in the 100K sample, randomly select 20K for validation using MLM\n",
    "remaining_after_100k = set(range(N)) - indices_100k\n",
    "indices_20k_val = set(random.sample(remaining_after_100k, VAL_SAMPLE_SIZE))\n",
    "\n",
    "# Step 5: The remaining abstracts not selected in any of the above datasets for Fine-Tuning\n",
    "indices_remaining = set(range(N)) - indices_100k - indices_20k_val\n",
    "\n",
    "# Create the five datasets using the selected indices\n",
    "df_100k = df_pre.select(list(indices_100k))\n",
    "df_10k = df_pre.select(list(indices_10k))\n",
    "df_50k = df_pre.select(list(indices_50k))\n",
    "df_20k_val = df_pre.select(list(indices_20k_val))\n",
    "df_remaining = df_pre.select(list(indices_remaining))\n",
    "\n",
    "# Save datasets to CSV files with headers\n",
    "df_100k.write.csv('../Datasets/dataset_100k.csv', header=True)\n",
    "df_10k.write.csv('../Datasets/dataset_10k.csv', header=True)\n",
    "df_50k.write.csv('../Datasets/dataset_50k.csv', header=True)\n",
    "df_20k_val.write.csv('../Datasets/dataset_20k_val.csv', header=True)\n",
    "df_remaining.write.csv('../Datasets/dataset_remaining.csv', header=True)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
